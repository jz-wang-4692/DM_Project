# DM_Project
This is a Columbia Data Mining class project, focusing on comparing the effect of different positional encoding methods for Vision Transformers (ViTs).

## Authors:
Chi Han, Jiazhen Wang, Ningping Wang, Lian Zhong

This project compares different positional encoding methods for Vision Transformers on CIFAR-10:
- Absolute Positional Encoding (APE)
- Rotary Positional Encoding (RoPE) variants from Heo et al., 2024
- Regular Relative Positional Encoding with 2L-1 learnable parameters
- Polynomial L1-distance based Relative Positional Encoding

## Acknowledgements

This project uses code from the RoPE-ViT repository:
@inproceedings{heo2024ropevit,
    title={Rotary Position Embedding for Vision Transformer},
    author={Heo, Byeongho and Park, Song and Han, Dongyoon and Yun, Sangdoo},
    year={2024},
    booktitle={European Conference on Computer Vision (ECCV)},
}

Parts of this code are based on the following repositories:
- RoPE-ViT (Apache-2.0): https://github.com/naver-ai/rope-vit
- CodeLlama (Meta): https://github.com/meta-llama/codellama

```
ðŸ“¦ Position Encoding Comparison
 â”£ ðŸ“‚ models                  # Model implementations
 â”ƒ â”£ ðŸ“‚ positional_encoding   # Positional encoding variants
 â”ƒ â”ƒ â”£ ðŸ“œ __init__.py         # Package initialization with imports
 â”ƒ â”ƒ â”£ ðŸ“œ Poly_RPE.py         # Polynomial RPE based on L1 distances
 â”ƒ â”ƒ â”£ ðŸ“œ RPE.py              # Standard RPE with 2L-1 learnable parameters 
 â”ƒ â”ƒ â”— ðŸ“œ fixed_rope_mixed.py # RoPE variants, adapted, reduced patch size
 â”ƒ â”£ ðŸ“œ model_factory.py      # Factory for creating ViT variants with different PEs
 â”ƒ â”— ðŸ“œ vit_base.py           # Base ViT implementation
 â”ƒ
 â”£ ðŸ“‚ training                # Training utilities
 â”ƒ â”— ðŸ“œ trainer.py            # Training and evaluation loops
 â”ƒ
 â”£ ðŸ“‚ utils                   # Utility functions
 â”ƒ â”— ðŸ“œ data.py               # CIFAR-10 data fetch, augment, split
 â”ƒ
 â”£ ðŸ“‚ rope-vit                # External RoPE implementation (from Heo et al.): https://github.com/naver-ai/rope-vit
 â”ƒ â”£ ðŸ“‚ models                # Original RoPE implementations
 â”ƒ â”£ ðŸ“‚ self-attn             # Self-attention mechanisms with RoPE
 â”ƒ â”— ðŸ“œ ...                   # Other files from original repo
 â”ƒ
 â”£ ðŸ“‚ output                  # NOT Existing now: Training outputs (created during runtime)
 â”ƒ â”£ ðŸ“‚ ape                   # Results for APE models
 â”ƒ â”£ ðŸ“‚ rope_axial            # Results for RoPE-Axial models
 â”ƒ â”£ ðŸ“‚ rope_mixed            # Results for RoPE-Mixed models
 â”ƒ â”£ ðŸ“‚ rpe                   # Results for standard RPE models
 â”ƒ â”— ðŸ“‚ polynomial_rpe        # Results for Polynomial RPE models
 â”ƒ
 â”£ ðŸ“œ main.py                 # Main script to run experiments, with visualization
 â”£ ðŸ“œ requirements.txt        # Project dependencies
 â”— ðŸ“œ README.md               # Project documentation
 ```